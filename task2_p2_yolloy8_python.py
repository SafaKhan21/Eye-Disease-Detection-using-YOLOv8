# -*- coding: utf-8 -*-
"""Task2_YOLLO8_(2)_(5) (5).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ba2vn4M4CTQW8Nps7ZuRRzAUj_cP97eX

# **1-Preprocssing Step**
"""

from google.colab import files
import zipfile
import os
import cv2
from skimage.feature import hog, local_binary_pattern
import numpy as np
import matplotlib.pyplot as plt

# A list containing the paths to the zip files‍
zip_paths =['/content/gg.zip']

# Unzipping each file in the list‍
for zip_path in zip_paths:
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall("/content/dataset10")

!pip install ultralytics
# Step 2: Install PyTorch and other necessary libraries
!pip install torch==1.13.1+cu113 torchvision==0.14.1+cu113 torchaudio==0.13.1 -f https://download.pytorch.org/whl/cu113/torch_stable.html

# Step 3: Install Detectron2 directly from GitHub (recommended for Colab)
!pip install git+https://github.com/facebookresearch/detectron2.git

# Step 4: Install other required libraries (YOLOv8, OpenCV, etc.)
!pip install ultralytics opencv-python tqdm matplotlib

# Step 5: Verify installation by checking the versions of key libraries
import torch
import torchvision
import detectron2
import ultralytics
import cv2
import matplotlib.pyplot as plt

# Check PyTorch and CUDA availability (for GPU acceleration)
print("PyTorch version:", torch.__version__)
print("CUDA Available:", torch.cuda.is_available())

# Check detectron2 version
print("Detectron2 version:", detectron2.__version__)

"""# 2-YOLLOY* model for detection and evaltion **"""

import glob
import time
import torch
import numpy as np
import matplotlib.pyplot as plt
from ultralytics import YOLO
import yaml
import cv2
import os

# Step 1: Dataset YAML
dataset_yaml = {
    'train': '/content/dataset10/train/images',
    'val': '/content/dataset10/valid/images',
    'test': '/content/dataset10/test/images',
    'nc': 3,  # Number of classes
    'names': ['Cup Disc', 'Optic Cup', 'Optic Disc']
}

with open('/content/dataset10.yaml', 'w') as file:
    yaml.dump(dataset_yaml, file)

# Step 2: Load YOLO model
model = YOLO('yolov8m.pt')  # Larger model for improved accuracy
model.train(data='/content/dataset10.yaml', epochs=50, imgsz=640)  # Increased epochs
print("Training completed!")

# Step 3: Helper Functions
def yolo_to_coco(yolo_labels, image_width, image_height):
    coco_boxes = []
    for label in yolo_labels:
        class_id, x_center, y_center, box_width, box_height = label
        x_min = (x_center - box_width / 2) * image_width
        y_min = (y_center - box_height / 2) * image_height
        x_max = (x_center + box_width / 2) * image_width
        y_max = (y_center + box_height / 2) * image_height
        coco_boxes.append({'bbox': [x_min, y_min, x_max, y_max], 'class': int(class_id)})
    return coco_boxes

def load_ground_truth(yolo_path, image_width, image_height):
    ground_truth_data = {}
    for txt_file in glob.glob(os.path.join(yolo_path, "*.txt")):
        with open(txt_file, "r") as f:
            yolo_labels = [
                list(map(float, line.strip().split())) for line in f.readlines()
            ]
            image_name = os.path.splitext(os.path.basename(txt_file))[0]
            ground_truth_data[image_name] = yolo_to_coco(yolo_labels, image_width, image_height)
    return ground_truth_data

def calculate_iou(box1, box2):
    x1 = max(box1[0], box2[0])
    y1 = max(box1[1], box2[1])
    x2 = min(box1[2], box2[2])
    y2 = min(box1[3], box2[3])

    intersection = max(0, x2 - x1) * max(0, y2 - y1)
    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])
    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])
    union = box1_area + box2_area - intersection

    return intersection / union if union > 0 else 0

# Step 4: Evaluate Model
results = model.predict(source='/content/dataset10/test/images', conf=0.4, save=True)  # Increased confidence threshold

image_width, image_height = 640, 640
yolo_annotations_path = '/content/dataset10/test/labels'
ground_truth_data = load_ground_truth(yolo_annotations_path, image_width, image_height)

iou_scores = []
inference_times = []

# Loop over all predictions and evaluate metrics
for result in results:
    start_time = time.time()

    # Predicted boxes, scores, and labels
    pred_boxes = result.boxes.xyxy.cpu().numpy()
    pred_labels = result.boxes.cls.cpu().numpy().astype(int)  # Convert to integers
    pred_scores = result.boxes.conf.cpu().numpy()

    # Extract the image name
    image_name = os.path.splitext(os.path.basename(result.path))[0]

    # Ground truth boxes and labels
    ground_truth_boxes = [
        item['bbox'] for item in ground_truth_data.get(image_name, [])
    ]
    ground_truth_labels = [
        item['class'] for item in ground_truth_data.get(image_name, [])
    ]

    # Compute IoU for each predicted box
    img = cv2.imread(result.path)
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    for pred_box, pred_label, pred_score in zip(pred_boxes, pred_labels, pred_scores):
        best_iou = 0
        for gt_box, gt_label in zip(ground_truth_boxes, ground_truth_labels):
            if pred_label == gt_label:  # Match class
                iou = calculate_iou(pred_box, gt_box)
                best_iou = max(best_iou, iou)
        if best_iou > 0:
            iou_scores.append(best_iou)
            print(f"Predicted Box: {pred_box}, IoU: {best_iou:.4f}")

        # Draw predicted box in green
        x_min, y_min, x_max, y_max = pred_box
        label = f"{dataset_yaml['names'][pred_label]} ({pred_score:.2f})"
        cv2.rectangle(img, (int(x_min), int(y_min)), (int(x_max), int(y_max)), (0, 255, 0), 2)
        cv2.putText(img, label, (int(x_min), int(y_min) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

    # Draw ground truth boxes in blue
    for gt_box, gt_label in zip(ground_truth_boxes, ground_truth_labels):
        x_min, y_min, x_max, y_max = gt_box
        label = dataset_yaml['names'][gt_label]
        cv2.rectangle(img, (int(x_min), int(y_min)), (int(x_max), int(y_max)), (255, 0, 0), 2)
        cv2.putText(img, label, (int(x_min), int(y_min) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)

    inference_times.append(time.time() - start_time)

    # Convert the image back to RGB before displaying with plt
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    # Display the image with boxes
    plt.figure(figsize=(10, 10))
    plt.imshow(img_rgb)
    plt.title(f"Predictions for {image_name}")
    plt.axis('off')
    plt.show()

# IoU Analysis
print("IoU Scores:")
print(iou_scores)
print(f"Average IoU: {np.mean(iou_scores):.4f}")

# Plot IoU Scores and Inference Times
plt.figure(figsize=(10, 5))

# IoU Scores Plot
plt.subplot(1, 2, 1)
plt.plot(range(len(iou_scores)), iou_scores, marker='o', linestyle='-', color='g')
plt.title("IoU Scores per Prediction")
plt.xlabel("Prediction Index")
plt.ylabel("IoU")
plt.grid(True)

# Inference Times Plot------------------------------
plt.subplot(1, 2, 2)
plt.plot(range(len(inference_times)), inference_times, marker='o', linestyle='-', color='b')
plt.title("Inference Time for Each Image")
plt.xlabel("Image Index")
plt.ylabel("Inference Time (seconds)")
plt.grid(True)

plt.tight_layout()
plt.show()

from sklearn.metrics import precision_score, recall_score, accuracy_score

# Initialize counters for precision, recall, and accuracy
all_gt_labels = []
all_pred_labels = []

# Loop over all predictions and evaluate metrics
for result in results:
    start_time = time.time()

    # Predicted boxes, scores, and labels
    pred_boxes = result.boxes.xyxy.cpu().numpy()
    pred_labels = result.boxes.cls.cpu().numpy().astype(int)  # Convert to integers
    pred_scores = result.boxes.conf.cpu().numpy()

    # Extract the image name
    image_name = os.path.splitext(os.path.basename(result.path))[0]

    # Ground truth boxes and labels
    ground_truth_boxes = [
        item['bbox'] for item in ground_truth_data.get(image_name, [])
    ]
    ground_truth_labels = [
        item['class'] for item in ground_truth_data.get(image_name, [])
    ]

    # Initialize flags to track correct predictions and ground truth
    match_found = set()  # To track which ground truth boxes have been matched

    # Compute IoU for each predicted box
    for pred_box, pred_label, pred_score in zip(pred_boxes, pred_labels, pred_scores):
        best_iou = 0
        matched_gt_label = None

        for i, (gt_box, gt_label) in enumerate(zip(ground_truth_boxes, ground_truth_labels)):
            if pred_label == gt_label:  # Match class
                iou = calculate_iou(pred_box, gt_box)
                if iou > best_iou:
                    best_iou = iou
                    matched_gt_label = gt_label
                    best_iou = iou

        # Append labels to respective lists for precision, recall, accuracy
        if matched_gt_label is not None and best_iou > 0.5:  # True Positive
            all_gt_labels.append(matched_gt_label)
            all_pred_labels.append(pred_label)
            match_found.add(matched_gt_label)  # Mark this ground truth as matched
        else:
            # False Positive (no match found)
            all_pred_labels.append(pred_label)
            all_gt_labels.append(0)  # 0 for no ground truth match

    # For ground truth labels not matched (False Negatives)
    for gt_label in ground_truth_labels:
        if gt_label not in match_found:
            all_gt_labels.append(gt_label)
            all_pred_labels.append(0)  # No predicted match for this ground truth

    # Compute inference time
    inference_times.append(time.time() - start_time)

# Flatten the lists to compute metrics
all_gt_labels = np.array(all_gt_labels)
all_pred_labels = np.array(all_pred_labels)

# Compute Precision, Recall, and Accuracy
precision = precision_score(all_gt_labels, all_pred_labels, average='macro', zero_division=1)
recall = recall_score(all_gt_labels, all_pred_labels, average='macro', zero_division=1)
accuracy = accuracy_score(all_gt_labels, all_pred_labels)

# Print metrics
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"Accuracy: {accuracy:.4f}")

# IoU Analysis
print("IoU Scores:")
print(iou_scores)
print(f"Average IoU: {np.mean(iou_scores):.4f}")

# Plot IoU Scores and Inference Times
plt.figure(figsize=(10, 5))